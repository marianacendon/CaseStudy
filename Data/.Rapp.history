base10to7 <-function(x){
i=0
sum=0
while(x%/%7!=0){
i=i+1
x<-x%/%7}
count <- integer(n)
for (i in 1:n) {
count[i] <- base10to7(i-1)
}}
}
p7(100)
p7
p7(5)
p7 <- function(n){
base10to7 <-function(x){
i=0
sum=0
while(x%/%7!=0){
sum <- sum+((x%%7)*(10^i))
i=i+1
x <- x%/%7
}
return(sum)
}
count[i] <- base10to7(i-1)
count <- integer(n)
for (i in 1:n) {
count[i] <- base10to7(i-1)
}}
p7(100)
p7 <- function(n){
base10to7 <- function (x){
i=0
sum=0
while(x%/%7!=0){
sum <- sum+((x%%7)*(10^i))
i=i+1
x < -x%/%7
}
sum <- sum+((x%%7)*(10^i))
return(sum)
}
count <- integer(n)
for (i in 1:n) {
count[i] <- base10to7(i-1)
}
p7(5)
p7
p7 <- function(n){#
base10to7 <- function(x){#
i=0#
sum=0#
while(x%/%7!=0){ #divides number by base 7 and returns an integer#
sum <- sum+((x%%7)*(10^i)) #%% gets remainder #
i=i+1#
x < -x%/%7#
}#
sum <- sum+((x%%7)*(10^i))#
return(sum)#
}#
count <- integer(n)#
for (i in 1:n) {#
count[i] <- base10to7(i-1)#
}#
count#
}
p7(15)
p7 <- function(n){
base10to7 <- function(x){#
i=0#
sum=0#
while(x%/%7!=0){ #divides number by base 7 and returns an integer#
sum <- sum+((x%%7)*(10^i)) #%% gets remainder #
i=i+1#
x < -x%/%7#
}#
sum <- sum+((x%%7)*(10^i))#
return(sum)#
}#
count <- integer(n)#
for (i in 1:n) {#
count[i] <- base10to7(i-1)#
}#
count#
}
p7(9)
base10to7 <- function(x){#
i=0#
sum=0#
while(x%/%7!=0){ #divides number by base 7 and returns an integer#
sum <- sum+((x%%7)*(10^i)) #%% gets remainder #
i=i+1#
x < -x%/%7#
}#
sum <- sum+((x%%7)*(10^i))#
return(sum)#
}#
count <- integer(n)#
for (i in 1:n) {#
count[i] <- base10to7(i-1)#
}#
count#
}
library("devtools")
install_github("Rfacebook", "pablobarbera", subdir="Rfacebook")
require("Rfacebook")
fb_oauth <- fbOAuth(app_id="123456789", app_secret="1A2B3C4D",extended_permissions = TRUE)
fb_oauth <- fbOAuth(app_id="981209695339223", app_secret="2f10ae6001d48699d3d9e8dad0a2bab7",extended_permissions = TRUE)
fb_oauth <- fbOAuth(app_id="981209695339223", app_secret="2f10ae6001d48699d3d9e8dad0a2bab7",extended_permissions = TRUE)
save(fb_oauth, file="fb_oauth")
load("fb_oauth")
me <- getUsers("me",token=fb_oauth)
me
my_likes <- getLikes(user="me", token=fb_oauth)
my_likes
library(XML)
install.packages(XML)
install.packages("XML")
library(XML)
fileURL<- "http://www.3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileURL, useInternal=TRUE) #Main work function of library XML, take URL and parse it into respective parts
fileURL<- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileURL, useInternal=TRUE) #Main work function of library XML, take URL and parse it into respective parts
rootNode <- xmlRoot(doc) #this gives the root of document
rootNode
#If just the value of rootNode is wanted
names(rootNode)
# root in this case is about food
xmlSapply(rootNode, xmlValue) # xmlSApply will take out out the xml value from the rootNode
xmlSApply(rootNode, xmlValue) # xmlSApply will take out out the xml value from the rootNode
#to look for menu items
xpathSApply(rootNode, "//name",xmlValue")
)
"")
xpathSApply(rootNode, "//name",xmlValue)
#for price
xpathSApply(rootNode, "//price",xmlValue)
#for ESPN website
filrURL <- "http://espn.go.com/nfl/team/stats/_/name/dal"
doc <- xmlTreeParse(fileURL, useInternal=TRUE) #Main work function of library XML, take URL and parse it into respective parts
players <- xpathSApply(doc,"//li[@class='name']",xmlValue)
players
list(players)
library(json)
options(nyt_as_key = 'e92b7b644c90476d917b64ce87e2791a')  ## copy and paste your API key here.
options()$nyt_as_key
installing.packages("rtimes")
install.packages("rtimes")
library(httr)
library(devtools)
resp <- GET('http://api.nytimes.com/svc/search/v2/articlesearch.json?facet_field=source&facet_filter=true&begin_date=20150105&end_date=20160105&api-key=sample-key')#
print(content(resp, 'parsed')$response$facets)
makeURL <- function(q=NULL, fq=NULL, begin_date=NULL, end_date=NULL, key=getOption("nyt_as_key"), page=0, sort=NULL, fl=NULL, hl=NULL, facet_field=NULL, facet_filter=NULL){
arglist <-(q=q, fq=fq, begin_date=begin_date, end_date=end_date, 'api-key'=key, page=page,#
                 sort=sort, fl=fl, hl=hl, facet_field=facet_field, facet_filter=facet_filter)
arglist <-(q=q, fq=fq, begin_date=begin_date, end_date=end_date, 'api-key'=key, page=page, sort=sort, fl=fl, hl=hl, facet_field=facet_field, facet_filter=facet_filter)
arglist <- list(q=q, fq=fq, begin_date=begin_date, end_date=end_date, 'api-key'=key, page=page, sort=sort, fl=fl, hl=hl, facet_field=facet_field, facet_filter=facet_filter)
makeURL <- function(q=NULL, fq=NULL, begin_date=NULL, end_date=NULL, key=getOption("nyt_as_key"), page=0, sort=NULL, fl=NULL, hl=NULL, facet_field=NULL, facet_filter=NULL){
arglist <- list(q=q, fq=fq, begin_date=begin_date, end_date=end_date, 'api-key'=key, page=page, sort=sort, fl=fl, hl=hl, facet_field=facet_field, facet_filter=facet_filter)
url <- 'http://api.nytimes.com/svc/search/v2/articlesearch.json?'
for(i in 1:length(arglist)){
if(is.null(unlist(arglist[i]))==F){
url <- paste0(url, '&', names(arglist[i]), '=', arglist[i])
}
}
return(url)
}
#example: generate query which will return all articles for one day
library(httr)
url <- makeURL(begin_date='20161025', end_date='20161026', key='sample-key', page=100 )
print(url)
getMeta <- function(url, pages=Inf, sleep=0.1, tryn=3)
{
art <- list()
i <- 1
e <- seq(-tryn, -tryn/2, length.out=tryn) # initialize list of failed pages with arbitrary negative numbers
while(i<=pages){
if(length(unique(e[(length(e)-(tryn-1)):length(e)]))==1) i<-i+1 ## attempt tryn times before moving on
tryget <- try({
urlp <-gsub('page=\d+', paste0('page=', i), url) ## get the next page
urlp <-gsub("page=\d+", paste0('page=', i), url) ## get the next page
getMeta <- function(url, pages=Inf, sleep=0.1, tryn=3)
{
art <- list()
i <- 1
e <- seq(-tryn, -tryn/2, length.out=tryn) # initialize list of failed pages with arbitrary negative numbers
while(i<=pages){
if(length(unique(e[(length(e)-(tryn-1)):length(e)]))==1) i<-i+1 ## attempt tryn times before moving on
tryget <- try({
urlp <-gsub("page=\d+", paste0('page=', i), url) ## get the next page
days <- gsub('-', '', seq(as.Date('2013-01-01'), Sys.Date()-1, by=1))
library(rmongo)
install.packages("RMongo")
library(rjson)
library(XML)
mongoCreds <- list(dbName='nyt', collection='articles1', host='ds063240.mongolab.com:63240', username='myUsername', password='myPassword')
key <- 'e92b7b644c90476d917b64ce87e2791a' # replace with your personal key
all <- list() ## persist all results in memory in addition to MongoDB... just in case.
for(d in days) {
url <- makeURL(begin_date=d, end_date=d, key=key) # generate URL
meta <- getMeta(url, pages=Inf, sleep=0.1) # extract metadata from NYT API
artxt <- getArticles(meta, n=Inf, sleep=0.1, overwrite=T, mongo=mongoCreds) # extract article text and write to MongoDB
print(paste0('day ',  d, ' complete at ', Sys.time()))
all <- append(all, artxt) # persist results
}
library(httr)
mongoCreds <- list(dbName='nyt', collection='articles1', host='ds063240.mongolab.com:63240', username='myUsername', password='myPassword')
key <- 'e92b7b644c90476d917b64ce87e2791a' # replace with your personal key
all <- append(all, artxt) # persist results
all <- list() ## persist all results in memory in addition to MongoDB... just in case.
for(d in days) {
url <- makeURL(begin_date=d, end_date=d, key=key) # generate URL
meta <- getMeta(url, pages=Inf, sleep=0.1) # extract metadata from NYT API
artxt <- getArticles(meta, n=Inf, sleep=0.1, overwrite=T, mongo=mongoCreds) # extract article text and write to MongoDB
print(paste0('day ',  d, ' complete at ', Sys.time()))
all <- append(all, artxt) # persist results
}
getArticles <- function(meta, n=Inf, overwrite=F, sleep=0.1, mongo=list(dbName, collection, host='127.0.0.1', username, password, ...)) {#
  metaArt <- meta#
  if(is.null(mongo$dbName)==F){#
    con <- mongoDbConnect(mongo$dbName, mongo$host)#
    try(authenticated <-dbAuthenticate(con, username=mongo$username, password=mongo$password))#
  }#
  if(overwrite==T) {artIndex <- 1:(min(n, length(meta)))#
  } else { #
    ii <-  which(sapply(meta, function(x) is.null(x[['bodyHTML']]))) ## get index of articles that have not been scraped yet#
    artIndex <- ii[1:min(n,length(ii))] ## take first n articles#
  }#
  e <- c(-3,-2,-1) # initialize failed list of articles with arbitrary negative numbers#
  i<-1#
  while(i<=length(artIndex)){#
    if(length(unique(e[(length(e)-2):length(e)]))==1) i <- i+1 ## if we tried and failed 3 times, move on#
    tryget <- try({#
      if(overwrite==T | (is.null(meta[[i]]$body)==T & overwrite==F)) {#
        p <- GET(meta[[i]]$web_url)#
        html <- content(p, 'text') ## metaArt[[i]]$bodyHTML <- content(p, 'text')#
        metaArt[[i]]$body <- parseArticleBody(html)#
        if(is.null(mongo$dbName)==F) dbInsertDocument(con, mongo$collection, toJSON(metaArt[[i]]))#
        if(i %% 10==0) print(paste0(i, ' articles scraped'))#
        i<-i+1#
      }#
    })#
    if(class(tryget)=='try-error') {#
      print(paste0(i, ' error - article not scraped'))#
      e <- c(e, i)#
      e <- e[(length(e)-2):length(e)]#
      Sys.sleep(0.5) ## probably scraping too fast -- slowing down#
    }#
    Sys.sleep(sleep)#
  }#
  return(metaArt)#
}
mongoCreds <- list(dbName='nyt', collection='articles1', host='ds063240.mongolab.com:63240', username='myUsername', password='myPassword')
key <- 'e92b7b644c90476d917b64ce87e2791a' # replace with your personal key
all <- list() ## persist all results in memory in addition to MongoDB... just in case.for(d in days) {
all <- list() ## persist all results in memory in addition to MongoDB... just in case.for(d in days) {
all <- list() ## persist all results in memory in addition to MongoDB... just in case.
for(d in days) {#
 url <- makeURL(begin_date=d, end_date=d, key=key) # generate URL#
 meta <- getMeta(url, pages=Inf, sleep=0.1) # extract metadata from NYT API#
 artxt <- getArticles(meta, n=Inf, sleep=0.1, overwrite=T, mongo=mongoCreds) # extract article text and write to MongoDB#
 print(paste0('day ',  d, ' complete at ', Sys.time()))#
 all <- append(all, artxt) # persist results#
 }
res <- as_search(q="Trump", begin_date = "20151001", end_date = '20161025')
out <- cg_billscosponsor(memberid='S001181', type='cosponsored')
library(rtimes)
out <- cg_billscosponsor(memberid='S001181', type='cosponsored')
options(nytimes_as_key = "e92b7b644c90476d917b64ce87e2791a")
res <- as_search(q="bailout", begin_date = "20081001", end_date = '20081201')
res <- as_search(q="Trump", begin_date = "20160901", end_date = '20161025')
res$copyright # copyright
res$meta # number of hits
res$data[[1]]
res$data[[1]]$snippet
library(rtimes)
setwd("~/Desktop/git_repositories/CaseStudy")
source("Data/gatherGDP.R")
GdpData <- read.csv("gdpdata.csv", sep=",", skip=4, stringsAsFactors=FALSE, header=TRUE)
colnames(GdpData)[1]<- "CountryCode"
colnames(GdpData)[2]<- "GDP.Ranking"
names(GdpData)
source("Data/gatherEduc.R")
getwd()
setwd("~/Desktop/git_repositories/CaseStudy")
source("Data/gatherEduc.R")
geData <- merge(GdpData, EducData, by = "CountryCode", all.y =TRUE)
sum(is.na(geData$GDP.Ranking))
genas <- is.na(geData$GDP.Ranking)
geData <- geData[!genas,]
geData$GDP.Ranking <- as.numeric(as.character(geData$GDP.Ranking))
library(plyr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(Hmisc)
ge.sorted <- geData[order(-geData$GDP.Ranking),]
mean(ge.sorted$GDP.Ranking[ge.sorted$Income.Group=="High income: OECD" ])
mean(ge.sorted$GDP.Ranking[ge.sorted$Income.Group=="High income: nonOECD"])
geData <- merge(GdpData, EducData, by = "CountryCode", all.y =TRUE)
geData$GDP.Ranking <- as.numeric(as.character(geData$GDP.Ranking))
geData <- merge(GdpData, EducData, by = "CountryCode", all.y =TRUE)
mean(ge.sorted$GDP.Ranking[ge.sorted$Income.Group=="High income: nonOECD"])
library(rtimes)
options(nytimes_as_key = "e92b7b644c90476d917b64ce87e2791a")
# This indicates what your key will be used for.
artsearch <- as_search(q="Trump", begin_date = "20161001", end_date = '20161026')
# This tells what phrase or word we are looking for and the begin and end dates to look for it.
artsearch$copyright
artsearch$meta
# This returns the number of hits
artsearch$data[[3]]
artsearch[[1]]
artsearch$data[[1]]
artsearch$data[]
artsearch$data[1:3]
artsearch$data[[1]]$snippet
library(XML)
fileURL <- "http://www.nytimes.com/video/us/politics/100000004698416/trump-responds-to-outrage-over-lewd-remarks.html"
article <- xmlTreeParse(fileURL, useinternal=TRUE)
article <- xmlTreeParse(fileURL, useinternal=TRUE)
article <- xmlTreeParse(fileURL, useInternal=TRUE)
library(jsonlite)
library(httr)
NYTdir =
setwd(NYTdir)
getKey <- function(hashCode)
{
keys = c(key1p, key2p, key3p, key1g, key2g, key3g, key1s, key2s, key3s, key1a, key2a, key3a)
numkeys = length(keys)
sleep = 0.1/numkeys
keyIndex = hashCode %% numkeys
keyIndex = keyIndex + 1
return(keys[keyIndex])
}
getQuickHits <- function(search, startYear, endYear)
{
root = "http://api.nytimes.com/svc/search/v2/articlesearch.json?fq="
startDate = paste(startYear,"0101",sep = "")
q()
